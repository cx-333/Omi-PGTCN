{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 处理 DNA.csv 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件，需要分批读取，由于文件过大 30G+\n",
    "dna_file = []\n",
    "dnas = pd.read_csv('../DNA.csv', index_col=0, iterator=True, chunksize=1000)\n",
    "for dna in dnas:\n",
    "    dna_file.append(dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th  :  []\n",
      "1 -th  :  []\n",
      "2 -th  :  []\n",
      "3 -th  :  []\n",
      "4 -th  :  []\n",
      "5 -th  :  []\n",
      "6 -th  :  []\n",
      "7 -th  :  []\n",
      "8 -th  :  []\n",
      "9 -th  :  []\n",
      "10 -th  :  []\n",
      "11 -th  :  []\n",
      "12 -th  :  []\n",
      "13 -th  :  []\n",
      "14 -th  :  []\n",
      "15 -th  :  []\n",
      "16 -th  :  []\n",
      "17 -th  :  []\n",
      "18 -th  :  []\n",
      "19 -th  :  []\n",
      "20 -th  :  []\n",
      "21 -th  :  []\n",
      "22 -th  :  []\n",
      "23 -th  :  []\n",
      "24 -th  :  []\n",
      "25 -th  :  []\n",
      "26 -th  :  []\n",
      "27 -th  :  []\n",
      "28 -th  :  []\n",
      "29 -th  :  []\n",
      "30 -th  :  []\n",
      "31 -th  :  []\n",
      "32 -th  :  []\n",
      "33 -th  :  []\n",
      "34 -th  :  []\n",
      "35 -th  :  []\n",
      "36 -th  :  []\n",
      "37 -th  :  []\n",
      "38 -th  :  []\n",
      "39 -th  :  []\n",
      "40 -th  :  []\n",
      "41 -th  :  []\n",
      "42 -th  :  []\n",
      "43 -th  :  []\n",
      "44 -th  :  []\n",
      "45 -th  :  []\n",
      "46 -th  :  []\n",
      "47 -th  :  []\n",
      "48 -th  :  []\n",
      "49 -th  :  []\n",
      "50 -th  :  []\n",
      "51 -th  :  []\n",
      "52 -th  :  []\n",
      "53 -th  :  []\n",
      "54 -th  :  []\n",
      "55 -th  :  []\n",
      "56 -th  :  []\n",
      "57 -th  :  []\n",
      "58 -th  :  []\n",
      "59 -th  :  []\n",
      "60 -th  :  []\n",
      "61 -th  :  []\n",
      "62 -th  :  []\n",
      "63 -th  :  []\n",
      "64 -th  :  []\n",
      "65 -th  :  []\n",
      "66 -th  :  []\n",
      "67 -th  :  []\n",
      "68 -th  :  []\n",
      "69 -th  :  []\n",
      "70 -th  :  []\n",
      "71 -th  :  []\n",
      "72 -th  :  []\n",
      "73 -th  :  []\n",
      "74 -th  :  []\n",
      "75 -th  :  []\n",
      "76 -th  :  []\n",
      "77 -th  :  []\n",
      "78 -th  :  []\n",
      "79 -th  :  []\n",
      "80 -th  :  []\n",
      "81 -th  :  []\n",
      "82 -th  :  []\n",
      "83 -th  :  []\n",
      "84 -th  :  []\n",
      "85 -th  :  []\n",
      "86 -th  :  []\n",
      "87 -th  :  []\n",
      "88 -th  :  []\n",
      "89 -th  :  []\n",
      "90 -th  :  []\n",
      "91 -th  :  []\n",
      "92 -th  :  []\n",
      "93 -th  :  []\n",
      "94 -th  :  []\n",
      "95 -th  :  []\n",
      "96 -th  :  []\n",
      "97 -th  :  []\n",
      "98 -th  :  []\n",
      "99 -th  :  []\n",
      "100 -th  :  []\n",
      "101 -th  :  []\n",
      "102 -th  :  []\n",
      "103 -th  :  []\n",
      "104 -th  :  []\n",
      "105 -th  :  []\n",
      "106 -th  :  []\n",
      "107 -th  :  []\n",
      "108 -th  :  []\n",
      "109 -th  :  []\n",
      "110 -th  :  []\n",
      "111 -th  :  []\n",
      "112 -th  :  []\n",
      "113 -th  :  []\n",
      "114 -th  :  []\n",
      "115 -th  :  []\n",
      "116 -th  :  []\n",
      "117 -th  :  []\n",
      "118 -th  :  []\n",
      "119 -th  :  []\n",
      "120 -th  :  []\n",
      "121 -th  :  []\n",
      "122 -th  :  []\n",
      "123 -th  :  []\n",
      "124 -th  :  []\n",
      "125 -th  :  []\n",
      "126 -th  :  []\n",
      "127 -th  :  []\n",
      "128 -th  :  []\n",
      "129 -th  :  []\n",
      "130 -th  :  []\n",
      "131 -th  :  []\n",
      "132 -th  :  []\n",
      "133 -th  :  []\n",
      "134 -th  :  []\n",
      "135 -th  :  []\n",
      "136 -th  :  []\n",
      "137 -th  :  []\n",
      "138 -th  :  []\n",
      "139 -th  :  []\n",
      "140 -th  :  []\n",
      "141 -th  :  []\n",
      "142 -th  :  []\n",
      "143 -th  :  []\n",
      "144 -th  :  []\n",
      "145 -th  :  []\n",
      "146 -th  :  []\n",
      "147 -th  :  []\n",
      "148 -th  :  []\n",
      "149 -th  :  []\n",
      "150 -th  :  []\n",
      "151 -th  :  []\n",
      "152 -th  :  []\n",
      "153 -th  :  []\n",
      "154 -th  :  []\n",
      "155 -th  :  []\n",
      "156 -th  :  []\n",
      "157 -th  :  []\n",
      "158 -th  :  []\n",
      "159 -th  :  []\n",
      "160 -th  :  []\n",
      "161 -th  :  []\n",
      "162 -th  :  []\n",
      "163 -th  :  []\n",
      "164 -th  :  []\n",
      "165 -th  :  []\n",
      "166 -th  :  []\n",
      "167 -th  :  []\n",
      "168 -th  :  []\n",
      "169 -th  :  []\n",
      "170 -th  :  []\n",
      "171 -th  :  []\n",
      "172 -th  :  []\n",
      "173 -th  :  []\n",
      "174 -th  :  []\n",
      "175 -th  :  []\n",
      "176 -th  :  []\n",
      "177 -th  :  []\n",
      "178 -th  :  []\n",
      "179 -th  :  []\n",
      "180 -th  :  []\n",
      "181 -th  :  []\n",
      "182 -th  :  []\n",
      "183 -th  :  []\n",
      "184 -th  :  []\n",
      "185 -th  :  []\n",
      "186 -th  :  []\n",
      "187 -th  :  []\n",
      "188 -th  :  []\n",
      "189 -th  :  []\n",
      "190 -th  :  []\n",
      "191 -th  :  []\n",
      "192 -th  :  []\n",
      "193 -th  :  []\n",
      "194 -th  :  []\n",
      "195 -th  :  []\n",
      "196 -th  :  []\n",
      "197 -th  :  []\n",
      "198 -th  :  []\n",
      "199 -th  :  []\n",
      "200 -th  :  []\n",
      "201 -th  :  []\n",
      "202 -th  :  []\n",
      "203 -th  :  []\n",
      "204 -th  :  []\n",
      "205 -th  :  []\n",
      "206 -th  :  []\n",
      "207 -th  :  []\n",
      "208 -th  :  []\n",
      "209 -th  :  []\n",
      "210 -th  :  []\n",
      "211 -th  :  []\n",
      "212 -th  :  []\n",
      "213 -th  :  []\n",
      "214 -th  :  []\n",
      "215 -th  :  []\n",
      "216 -th  :  []\n",
      "217 -th  :  []\n",
      "218 -th  :  []\n",
      "219 -th  :  []\n",
      "220 -th  :  []\n",
      "221 -th  :  []\n",
      "222 -th  :  []\n",
      "223 -th  :  []\n",
      "224 -th  :  []\n",
      "225 -th  :  []\n",
      "226 -th  :  []\n",
      "227 -th  :  []\n",
      "228 -th  :  []\n",
      "229 -th  :  []\n",
      "230 -th  :  []\n",
      "231 -th  :  []\n",
      "232 -th  :  []\n",
      "233 -th  :  []\n",
      "234 -th  :  []\n",
      "235 -th  :  []\n",
      "236 -th  :  []\n",
      "237 -th  :  []\n",
      "238 -th  :  []\n",
      "239 -th  :  []\n",
      "240 -th  :  []\n",
      "241 -th  :  []\n",
      "242 -th  :  []\n",
      "243 -th  :  []\n",
      "244 -th  :  []\n",
      "245 -th  :  []\n",
      "246 -th  :  []\n",
      "247 -th  :  []\n",
      "248 -th  :  []\n",
      "249 -th  :  []\n",
      "250 -th  :  []\n",
      "251 -th  :  []\n",
      "252 -th  :  []\n",
      "253 -th  :  []\n",
      "254 -th  :  []\n",
      "255 -th  :  []\n",
      "256 -th  :  []\n",
      "257 -th  :  []\n",
      "258 -th  :  []\n",
      "259 -th  :  []\n",
      "260 -th  :  []\n",
      "261 -th  :  []\n",
      "262 -th  :  []\n",
      "263 -th  :  []\n",
      "264 -th  :  []\n",
      "265 -th  :  []\n",
      "266 -th  :  []\n",
      "267 -th  :  []\n",
      "268 -th  :  []\n",
      "269 -th  :  []\n",
      "270 -th  :  []\n",
      "271 -th  :  []\n",
      "272 -th  :  []\n",
      "273 -th  :  []\n",
      "274 -th  :  []\n",
      "275 -th  :  []\n",
      "276 -th  :  []\n",
      "277 -th  :  []\n",
      "278 -th  :  []\n",
      "279 -th  :  []\n",
      "280 -th  :  []\n",
      "281 -th  :  []\n",
      "282 -th  :  []\n",
      "283 -th  :  []\n",
      "284 -th  :  []\n",
      "285 -th  :  []\n",
      "286 -th  :  []\n",
      "287 -th  :  []\n",
      "288 -th  :  []\n",
      "289 -th  :  []\n",
      "290 -th  :  []\n",
      "291 -th  :  []\n",
      "292 -th  :  []\n",
      "293 -th  :  []\n",
      "294 -th  :  []\n",
      "295 -th  :  []\n",
      "296 -th  :  []\n",
      "297 -th  :  []\n",
      "298 -th  :  []\n",
      "299 -th  :  []\n",
      "300 -th  :  []\n",
      "301 -th  :  []\n",
      "302 -th  :  []\n",
      "303 -th  :  []\n",
      "304 -th  :  []\n",
      "305 -th  :  []\n",
      "306 -th  :  []\n",
      "307 -th  :  []\n",
      "308 -th  :  []\n",
      "309 -th  :  []\n",
      "310 -th  :  []\n",
      "311 -th  :  []\n",
      "312 -th  :  []\n",
      "313 -th  :  []\n",
      "314 -th  :  []\n",
      "315 -th  :  []\n",
      "316 -th  :  []\n",
      "317 -th  :  []\n",
      "318 -th  :  []\n",
      "319 -th  :  []\n",
      "320 -th  :  []\n",
      "321 -th  :  []\n",
      "322 -th  :  []\n",
      "323 -th  :  []\n",
      "324 -th  :  []\n",
      "325 -th  :  []\n",
      "326 -th  :  []\n",
      "327 -th  :  []\n",
      "328 -th  :  []\n",
      "329 -th  :  []\n",
      "330 -th  :  []\n",
      "331 -th  :  []\n",
      "332 -th  :  []\n",
      "333 -th  :  []\n",
      "334 -th  :  []\n",
      "335 -th  :  []\n",
      "336 -th  :  []\n",
      "337 -th  :  []\n",
      "338 -th  :  []\n",
      "339 -th  :  []\n",
      "340 -th  :  []\n",
      "341 -th  :  []\n",
      "342 -th  :  []\n",
      "343 -th  :  []\n",
      "344 -th  :  []\n",
      "345 -th  :  []\n",
      "346 -th  :  []\n",
      "347 -th  :  []\n",
      "348 -th  :  []\n",
      "349 -th  :  []\n",
      "350 -th  :  []\n",
      "351 -th  :  []\n",
      "352 -th  :  []\n",
      "353 -th  :  []\n",
      "354 -th  :  []\n",
      "355 -th  :  []\n",
      "356 -th  :  []\n",
      "357 -th  :  []\n",
      "358 -th  :  []\n",
      "359 -th  :  []\n",
      "360 -th  :  []\n",
      "361 -th  :  []\n",
      "362 -th  :  []\n",
      "363 -th  :  []\n",
      "364 -th  :  []\n",
      "365 -th  :  []\n",
      "366 -th  :  []\n",
      "367 -th  :  []\n",
      "368 -th  :  []\n",
      "369 -th  :  []\n",
      "370 -th  :  []\n",
      "371 -th  :  []\n",
      "372 -th  :  []\n",
      "373 -th  :  []\n",
      "374 -th  :  []\n",
      "375 -th  :  []\n",
      "376 -th  :  []\n",
      "377 -th  :  []\n",
      "378 -th  :  []\n",
      "379 -th  :  []\n",
      "380 -th  :  []\n",
      "381 -th  :  []\n",
      "382 -th  :  []\n",
      "383 -th  :  []\n",
      "384 -th  :  []\n",
      "385 -th  :  []\n",
      "386 -th  :  []\n",
      "387 -th  :  []\n",
      "388 -th  :  []\n",
      "389 -th  :  []\n",
      "390 -th  :  []\n",
      "391 -th  :  []\n",
      "392 -th  :  []\n",
      "393 -th  :  []\n",
      "394 -th  :  []\n",
      "395 -th  :  []\n",
      "396 -th  :  []\n"
     ]
    }
   ],
   "source": [
    "# 过滤 NA/0 值超过 70% 的行和列\n",
    "\n",
    "len_dna_file = len(dna_file)\n",
    "len_col = len(dna_file[0].columns)\n",
    "\n",
    "for l in range(len_dna_file):\n",
    "    drop_index = [ ]\n",
    "    for i in range(len(dna_file[l])):\n",
    "        if(dna_file[l].iloc[i, :].isna().sum() >= len_col*0.7):\n",
    "            drop_index.append(dna_file[l].index[i])\n",
    "    dna_file[l].drop(drop_index, axis=0, inplace=True)\n",
    "    print(str(l)+' -th ', ': ', drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基因数量：  396065\n",
      "将被过滤的样本：  []\n"
     ]
    }
   ],
   "source": [
    "len_index = len(dna_file[0].index) * (len_dna_file-1) + len(dna_file[-1].index)\n",
    "print(\"基因数量： \", len_index)\n",
    "drop_col = [ ]\n",
    "for i in range(len_col):\n",
    "    na_sum = 0\n",
    "    for l in range(len_dna_file):\n",
    "        na_sum += dna_file[l].iloc[:, i].isna().sum()\n",
    "    if(na_sum >= len_index*0.7):\n",
    "        drop_col.append(dna_file[0].index[i])\n",
    "print(\"将被过滤的样本： \", drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将剩下的 NA 值以该行的均值替代\n",
    "for l in range(len_dna_file):\n",
    "    for i in range(len(dna_file[l].index)):\n",
    "        dna_file[l].iloc[i, :].fillna(value=dna_file[l].iloc[i, :].mean(), axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此时NA值的数量为:  0\n"
     ]
    }
   ],
   "source": [
    "# 确认是否还存在 NA 值\n",
    "test_na_Sum = 0\n",
    "\n",
    "for l in range(len_dna_file):\n",
    "    test_na_Sum += sum(dna_file[l].isna().sum())\n",
    "\n",
    "print(\"此时NA值的数量为: \", test_na_Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存文件\n",
    "dna_file[0].to_csv('DNA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(1, len_dna_file):\n",
    "    dna_file[l].to_csv('DNA.csv', header=False, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 找出三个组学数据中互相包含的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA 样本数：  9664 \n",
      " RNA 样本数： 11069 \n",
      " RPPA 样本数： 7754\n"
     ]
    }
   ],
   "source": [
    "# 获取每个组学所包含的样本数\n",
    "\n",
    "dna_samples = dna_file[0].columns.tolist()\n",
    "rna_samples = pd.read_csv('RNA.csv', index_col=0).columns.tolist()\n",
    "rppa_samples = pd.read_csv('RPPA.csv', index_col=0).columns.tolist()\n",
    "print(\"DNA 样本数： \", len(dna_samples), '\\n', \"RNA 样本数：\", len(rna_samples), '\\n', \"RPPA 样本数：\", len(rppa_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相互包含的样本数为: 6133\n"
     ]
    }
   ],
   "source": [
    "# 过滤各个组学互不包含的的样本，留下相同的样本\n",
    "co_samples = [sample for sample in rna_samples if sample in dna_samples and sample in rppa_samples]\n",
    "print(\"相互包含的样本数为: {}\".format(len(co_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_id = pd.read_csv('../data/common_samples.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in co_samples: assert sample in tumor_id['Samples'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samples.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(tumor_id)):\n",
    "        temp = tumor_id[['Samples', 'Tumor_id']].iloc[i, :].tolist()\n",
    "        f.write(temp[0]+' '+str(temp[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 **.csv文件，只保留相互包含的样本\n",
    "rna_file = pd.read_csv('RNA.csv', index_col=0)\n",
    "rppa_file = pd.read_csv('RPPA.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_file = rna_file[co_samples]\n",
    "rna_file.to_csv('rna.csv')\n",
    "rppa_file = rppa_file[co_samples]\n",
    "rppa_file.to_csv('rppa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dna_file[0][co_samples]\n",
    "temp.to_csv('dna.csv')\n",
    "for l in range(1, len_dna_file):\n",
    "    temp = dna_file[l][co_samples]\n",
    "    temp.to_csv('dna.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 拆分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dict = {}\n",
    "for i in range(33):\n",
    "    samples_dict[i] = tumor_id.loc[tumor_id['Tumor_id'] == i]['Samples'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取各类肿瘤 样本数量\n",
    "classer_num = []\n",
    "for i in samples_dict.values():\n",
    "    classer_num.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肿瘤名： Testicular Germ Cell Tumors, 标签： 0, 训练集数量：48，验证集数量：36， 测试集数量：38\n",
      "肿瘤名： Head and Neck squamous cell carcinoma, 标签： 1, 训练集数量：135，验证集数量：101， 测试集数量：102\n",
      "肿瘤名： Breast invasive carcinoma, 标签： 2, 训练集数量：250，验证集数量：187， 测试集数量：189\n",
      "肿瘤名： Bladder Urothelial Carcinoma, 标签： 3, 训练集数量：167，验证集数量：125， 测试集数量：127\n",
      "肿瘤名： Lymphoid Neoplasm Diffuse Large B-cell Lymphoma, 标签： 4, 训练集数量：13，验证集数量：9， 测试集数量：11\n",
      "肿瘤名： Cervical squamous cell carcinoma and endocervical adenocarcinoma, 标签： 5, 训练集数量：67，验证集数量：50， 测试集数量：51\n",
      "肿瘤名： Liver hepatocellular carcinoma, 标签： 6, 训练集数量：71，验证集数量：53， 测试集数量：54\n",
      "肿瘤名： Rectum adenocarcinoma, 标签： 7, 训练集数量：29，验证集数量：21， 测试集数量：23\n",
      "肿瘤名： Kidney renal clear cell carcinoma, 标签： 8, 训练集数量：129，验证集数量：96， 测试集数量：98\n",
      "肿瘤名： Stomach adenocarcinoma, 标签： 9, 训练集数量：117，验证集数量：87， 测试集数量：89\n",
      "肿瘤名： Colon adenocarcinoma, 标签： 10, 训练集数量：95，验证集数量：71， 测试集数量：73\n",
      "肿瘤名： Sarcoma, 标签： 11, 训练集数量：87，验证集数量：65， 测试集数量：67\n",
      "肿瘤名： Esophageal carcinoma, 标签： 12, 训练集数量：49，验证集数量：37， 测试集数量：38\n",
      "肿瘤名： Pheochromocytoma and Paraganglioma, 标签： 13, 训练集数量：32，验证集数量：24， 测试集数量：26\n",
      "肿瘤名： Glioblastoma multiforme, 标签： 14, 训练集数量：12，验证集数量：9， 测试集数量：11\n",
      "肿瘤名： Prostate adenocarcinoma, 标签： 15, 训练集数量：139，验证集数量：104， 测试集数量：106\n",
      "肿瘤名： Kidney Chromophobe, 标签： 16, 训练集数量：24，验证集数量：18， 测试集数量：20\n",
      "肿瘤名： Uterine Corpus Endometrial Carcinoma, 标签： 17, 训练集数量：117，验证集数量：87， 测试集数量：89\n",
      "肿瘤名： Skin Cutaneous Melanoma, 标签： 18, 训练集数量：139，验证集数量：104， 测试集数量：105\n",
      "肿瘤名： Kidney renal papillary cell carcinoma, 标签： 19, 训练集数量：99，验证集数量：74， 测试集数量：76\n",
      "肿瘤名： Uveal Melanoma, 标签： 20, 训练集数量：4，验证集数量：3， 测试集数量：5\n",
      "肿瘤名： Pancreatic adenocarcinoma, 标签： 21, 训练集数量：38，验证集数量：29， 测试集数量：30\n",
      "肿瘤名： Lung adenocarcinoma, 标签： 22, 训练集数量：21，验证集数量：15， 测试集数量：17\n",
      "肿瘤名： Uterine Carcinosarcoma, 标签： 23, 训练集数量：19，验证集数量：14， 测试集数量：15\n",
      "肿瘤名： Cholangiocarcinoma, 标签： 24, 训练集数量：12，验证集数量：9， 测试集数量：9\n",
      "肿瘤名： Thymoma, 标签： 25, 训练集数量：45，验证集数量：33， 测试集数量：35\n",
      "肿瘤名： Lung squamous cell carcinoma, 标签： 26, 训练集数量：10，验证集数量：7， 测试集数量：9\n",
      "肿瘤名： Chronic Myelogenous Leukemia, 标签： 27, 训练集数量：90，验证集数量：67， 测试集数量：68\n",
      "肿瘤名： Thyroid carcinoma, 标签： 28, 训练集数量：147，验证集数量：110， 测试集数量：111\n",
      "肿瘤名： Mesothelioma, 标签： 29, 训练集数量：24，验证集数量：18， 测试集数量：19\n",
      "肿瘤名： Brain Lower Grade Glioma, 标签： 30, 训练集数量：172，验证集数量：129， 测试集数量：131\n",
      "肿瘤名： Ovarian serous cystadenocarcinoma, 标签： 31, 训练集数量：21，验证集数量：15， 测试集数量：17\n",
      "肿瘤名： Adrenocortical carcinoma, 标签： 32, 训练集数量：18，验证集数量：13， 测试集数量：14\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "train_ref_num, va_ref_num, test_ref_num = [], [], []\n",
    "\n",
    "train_samples = {}    # 40%\n",
    "validation_samples = {}  # 30%\n",
    "test_samples = {}    # 30%\n",
    "for idx, i in enumerate(classer_num):\n",
    "    tr_n = int(i*0.4)\n",
    "    train_ref_num.append(tr_n)\n",
    "    v_n = int(i*0.3)\n",
    "    va_ref_num.append(v_n)\n",
    "    te_n = i - tr_n - v_n\n",
    "    test_ref_num.append(te_n)\n",
    "    train_samples[idx] = samples_dict[idx][0:tr_n]\n",
    "    validation_samples[idx] = samples_dict[idx][tr_n: tr_n+v_n]\n",
    "    test_samples[idx] = samples_dict[idx][tr_n+v_n: ]\n",
    "    print(\"肿瘤名： {}, 标签： {}, 训练集数量：{}，验证集数量：{}， 测试集数量：{}\"\n",
    "    .format(tumor_id[(tumor_id['Tumor_id'] == idx)].index.tolist()[0], idx, tr_n, v_n, te_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储划分的数据（未进行过采样处理）\n",
    "with open('origin_classify_samples.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Train samples data: \\n\")\n",
    "    for i in range(len(train_samples)):\n",
    "        for j in train_samples[i]:\n",
    "            f.write(j+' '+str(i)+'\\n')\n",
    "    f.write(\"Validation samples data: \\n\")\n",
    "    for i in range(len(validation_samples)):\n",
    "        for j in validation_samples[i]:\n",
    "            f.write(j+' '+str(i)+'\\n')\n",
    "    f.write(\"Test samples data: \\n\")\n",
    "    for i in range(len(test_samples)):\n",
    "        for j in test_samples[i]:\n",
    "            f.write(j+' '+str(i)+'\\n')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对少数样本过采样， 减小数据不平衡带来的影响\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_num, val_max_num, test_max_num = np.max(train_ref_num), np.max(va_ref_num), np.max(test_ref_num)\n",
    "for i in range(len(samples_dict)):\n",
    "    \n",
    "    if(train_ref_num[i] < train_max_num):\n",
    "        temp = len(train_samples[i])\n",
    "        for j in range(train_max_num - temp):\n",
    "            train_samples[i].append(choice(train_samples[i]))\n",
    "\n",
    "    if(va_ref_num[i] < val_max_num):\n",
    "        temp = len(validation_samples[i])\n",
    "        for j in range(val_max_num - temp):\n",
    "            validation_samples[i].append(choice(validation_samples[i]))  \n",
    "\n",
    "    if(test_ref_num[i] < test_max_num):\n",
    "        temp = len(test_samples[i])\n",
    "        for j in range(test_max_num - temp):\n",
    "            test_samples[i].append(choice(test_samples[i]))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验过采样是否成功\n",
    "for i in range(len(train_samples)): assert len(train_samples[i]) == train_max_num\n",
    "for i in range(len(validation_samples)): assert len(validation_samples[i]) == val_max_num\n",
    "for i in range(len(test_samples)): assert len(test_samples[i]) == test_max_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存采样之后的数据集\n",
    "with open('../my_dataset/train.txt', 'w', encoding='utf-8') as train_f, \\\n",
    "     open('../my_dataset/validation.txt', 'w', encoding='utf-8') as validation_f, \\\n",
    "     open('../my_dataset/test.txt', 'w', encoding='utf-8') as test_f:\n",
    "    for i in range(len(train_samples)):\n",
    "        for j in train_samples[i]:\n",
    "            train_f.write(j+' '+str(i)+'\\n')\n",
    "    for i in range(len(validation_samples)):\n",
    "        for j in validation_samples[i]:\n",
    "            validation_f.write(j+' '+str(i)+'\\n')   \n",
    "    for i in range(len(test_samples)):\n",
    "        for j in test_samples[i]:\n",
    "            test_f.write(j+' '+str(i)+'\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 制作数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_list = [s for i in range(len(train_samples)) for s in train_samples[i]]\n",
    "validation_samples_list = [s for i in range(len(validation_samples)) for s in validation_samples[i]]\n",
    "test_samples_list = [s for i in range(len(test_samples)) for s in test_samples[i]]\n",
    "\n",
    "for sample in co_samples:\n",
    "    temp_dna = dna_file[0][sample].tolist()\n",
    "    for l in range(1, len(dna_file)):\n",
    "        temp_dna.append(dna_file[l][sample].tolist())\n",
    "    temp_dna = str(temp_dna).replace('[', '').replace(']', '')\n",
    "    temp_rna = str(rna_file[sample].tolist()).replace('[', '').replace(']', '')\n",
    "    temp_rppa = str(rppa_file[sample].tolist()).replace('[', '').replace(']', '')    \n",
    "    \n",
    "    if sample in train_samples_list:\n",
    "        with open('../data/train/dna/{}.txt'.format(sample), 'w', encoding='utf-8') as dna_f, \\\n",
    "             open('../data/train/rna/{}.txt'.format(sample), 'w', encoding='utf-8') as rna_f, \\\n",
    "             open('../data/train/rppa/{}.txt'.format(sample), 'w', encoding='utf-8') as rppa_f:\n",
    "             dna_f.write(temp_dna)\n",
    "             rna_f.write(temp_rna)\n",
    "             rppa_f.write(temp_rppa)\n",
    "    elif sample in validation_samples_list:\n",
    "        with open('../data/validation/dna/{}.txt'.format(sample), 'w', encoding='utf-8') as dna_f, \\\n",
    "             open('../data/validation/rna/{}.txt'.format(sample), 'w', encoding='utf-8') as rna_f, \\\n",
    "             open('../data/validation/rppa/{}.txt'.format(sample), 'w', encoding='utf-8') as rppa_f:\n",
    "             dna_f.write(temp_dna)\n",
    "             rna_f.write(temp_rna)\n",
    "             rppa_f.write(temp_rppa)       \n",
    "    elif sample in test_samples_list:\n",
    "        with open('../data/test/dna/{}.txt'.format(sample), 'w', encoding='utf-8') as dna_f, \\\n",
    "             open('../data/test/rna/{}.txt'.format(sample), 'w', encoding='utf-8') as rna_f, \\\n",
    "             open('../data/test/rppa/{}.txt'.format(sample), 'w', encoding='utf-8') as rppa_f:\n",
    "             dna_f.write(temp_dna)\n",
    "             rna_f.write(temp_rna)\n",
    "             rppa_f.write(temp_rppa)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *附：清除DNA中未匹配染色体的探针"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\cx\\paper\\code\\data_preprocessnig\\3_dna_process.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/cx/paper/code/data_preprocessnig/3_dna_process.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dna_file \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/cx/paper/code/data_preprocessnig/3_dna_process.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dnas \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mDNA.csv\u001b[39m\u001b[39m'\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, iterator\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, chunksize\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/cx/paper/code/data_preprocessnig/3_dna_process.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m dna \u001b[39min\u001b[39;00m dnas:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/cx/paper/code/data_preprocessnig/3_dna_process.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     dna_file\u001b[39m.\u001b[39mappend(dna)\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1188\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1187\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1188\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_chunk()\n\u001b[0;32m   1189\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1285\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow)\n\u001b[1;32m-> 1285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nrows\u001b[39m=\u001b[39;49msize)\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1255\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1253\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1254\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1255\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m   1256\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    226\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:817\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Apps\\Anaconda\\install\\envs\\chenxin\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# 读取文件，需要分批读取，由于文件过大 30G+\n",
    "dna_file = []\n",
    "dnas = pd.read_csv('DNA.csv', index_col=0, iterator=True, chunksize=1000)\n",
    "for dna in dnas:\n",
    "    dna_file.append(dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg38 = pd.read_csv('illuminaMethyl450_hg38_GDC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('chenxin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f252631d5e874d75b643969ba75b9263e6dedc03bbf02e16a9f8286696a5b9a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
